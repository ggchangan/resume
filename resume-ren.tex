% !TEX program = xelatex

\documentclass{resume}
%\usepackage{zh_CN-Adobefonts_external} % Simplified Chinese Support using external fonts (./fonts/zh_CN-Adobe/)
%\usepackage{zh_CN-Adobefonts_internal} % Simplified Chinese Support using system fonts

\begin{document}
\pagenumbering{gobble} % suppress displaying page number

\name{Ren Zhang}

\basicInfo{
  \email{xueluowuhen.2007@163.com} \textperiodcentered\ 
  \phone{(+86) 185-0097-2851} \textperiodcentered\ }

\section{\faGraduationCap\ Education}
\datedsubsection{\textbf{Beihang University (BUAA)}, Beijing}{September 2011 -- March 2015}
\textit{Master of Engineering}\ Computer Science and Technology
\datedsubsection{\textbf{Northeast Normal University}, Jilin}{September 2006 -- July 2010}
\textit{Bachelor of Engineering}\ Software Engineering

\section{\faUsers\ Professional Skill}
\begin{itemize}
  \item Proficient in Golang, Java, C++; familiar with Python, Shell, Scala, Matlab, MFC, STL, and other languages.
  \item Proficient in basic data structures and algorithms, design patterns, multi-threaded programming, distributed system concepts, and system design principles. 
  \item Familiar with backend design methodologies, Gin framework, common Golang libraries, Golang testing frameworks, and extensive experience in implementing backend systems using Golang. 
  \item Familiar with Elasticsearch, ClickHouse; knowledgeable in Lucene, Kafka, Redis, Spark, and the Hadoop ecosystem.
  \item Proficient in Docker, Kubernetes, Terraform, Salt; knowledgeable in Prometheus and Jenkins. 
\end{itemize}

\section{\faUsers\ Internship/Project Experience}
\datedsubsection{\textbf{FreeWheel}, Beijing}{November 2017 -- May 2023}
\role{Senior Software Development Engineer}{}
\subsection*{ClickHouse Data Analysis Platform (June 2020 -- May 2023)}
\begin{enumerate}
    \item \textbf{Project Overview:}
    \begin{itemize}
        \item Responsible for handling batch and real-time data from various applications in the ClickHouse Data Analysis Platform.
        \item Provided real-time data query services for teams such as Forecast, Identity, and Insight.
        \item ClickHouse cluster with 24 machines, 300TB batch data, daily increase of 2.5TB, 3 billion rows/batch, 8TB real-time data, and daily increase of 200GB.
    \end{itemize}

    \item \textbf{Key Responsibilities:}
    \begin{itemize}
        \item Managed all aspects of batch data import: scheduling, design, development, code review, deployment, etc.
        \item Collaborated with multiple teams for integration of different data sources, development of performance testing platforms, and training.
        \item Conducted research and sharing on ClickHouse functionality and SQL performance optimization (added indexes, projections, etc.).
        \item Involved in CI/CD, monitoring systems, data validation systems, and various daily maintenance tasks.
    \end{itemize}

    \item \textbf{Tools and Key Technologies:}
    \begin{itemize}
        \item ClickHouse, ClickHouse-exporter, Airflow, Golang, K8s, Docker, Helm, Jenkins, EKS, S3, ORC.
    \end{itemize}

    \item \textbf{Achievements:}
    \begin{itemize}
      \item Abstracted the Golang backend architecture, enabling the customization of web backend services in just two person-days.
      \item Provided distributed data import and refresh services for teams such as Forecast, Identity, and Insight. The system exhibits excellent scalability, reducing the integration cycle for new data sources from at least 5 working days to 2 working days. 
      \item Utilized a distributed import tool for task partitioning, resource management, and concurrency control, resulting in a 50\% reduction in AWS EC2 costs. Additionally, enhanced data import stability by changing the retry granularity from batch-level to table-level.
      \item Shared general principles and functionalities of ClickHouse, improving team members' understanding of ClickHouse features. Promoted the use of ClickHouse Explain, ClickHouse Projection, secondary indexes, and other features across multiple teams. Shared methods for ClickHouse SQL performance monitoring and optimization, transforming high-frequency and slow-running SQL queries from minutes to seconds (5m-10s). Optimized data product response capabilities and reduced ClickHouse workload.
      \item Developed a performance testing platform for automating ClickHouse version upgrades or new data integrations. Reduced the time spent on routine tasks from 5 person-days to 2 person-days and provided visual performance comparisons and analysis.
      \item Responsible for optimizing and developing CI/CD, data validation systems, and monitoring systems for DataLoaderManager. This included modifications to ClickHouse-explore source code, providing SQL performance monitoring metrics, and creating a dashboard for slow SQL queries.
  \end{itemize}
\end{enumerate}
\subsection*{Analytics Data Product (November 2017 -- May 2020)}
\begin{enumerate}
    \item \textbf{Project Overview:}
    \begin{itemize}
        \item Provided data reporting services through Analytics and DataFeed products, supporting periodic report exports.
        \item Developed real-time viewing products like Analytics, SuperAnalytics, DataFeed, CampaignInsight, MarketInsight, and related monitoring and data validation services.
    \end{itemize}

    \item \textbf{Key Responsibilities:}
    \begin{itemize}
        \item Participated in requirements review and interface definition for multiple modules of Analytics, DataFeed, SuperAnalytics, Doraemon, CampaignInsight, MarketInsight, etc.
        \item Promoted the use of the Gin web backend framework, Golang testing framework, and Airflow within the team.
        \item Contributed to the promotion and implementation of CI/CD deployment using Salt/Terraform and later EKS for multiple projects.
        \item Optimized Analytics based on Domain table Looker data modeling and corresponding SQL performance.
    \end{itemize}

    \item \textbf{Tools and Key Technologies:}
    \begin{itemize}
        \item Golang, Looker, Presto, Terraform, Salt, K8s, AWS, Gin, MySQL, Python, Airflow, Reactor, PQM.
    \end{itemize}

    \item \textbf{Achievements:}
    \begin{itemize}
        \item Developed products including DataFeed, CampaignInsight, SuperAnalytics, Doraemon, addressing various data reporting needs.
        \item Supplemented Analytics with DataFeed to address the lack of hourly granularity data.
        \item Introduced new products (SuperAnalytics) by removing timezone limitations, expanding Analytics capabilities across the company.
        \item Successfully stabilized Disney's customer base with CampaignInsight.
        \item Developed a new monitoring system (Doraemon) and improved ESC processing capabilities.
        \item Enhanced Analytics core functionality, introducing multi-format export (csv to xlsx) and data formatting.
        \item Promoted the use of the Gin framework, improving Golang backend development speed.
        \item Optimized SQL performance, saving time, reducing Presto cluster utilization, and saving daily costs.
        \item Abstracted the Golang backend structure, allowing for the development of customized web backend services in as little as two person-days.
    \end{itemize}
\end{enumerate}

\datedsubsection{\textbf{Beijing Qihoo Certeon Technology Co., Ltd.}, Beijing}{April 2015 -- November 2017}
\role{Big Data Platform Development Engineer}{}
Data Cleansing Platform
\begin{itemize}
\item Project Introduction: The Data Cleansing Platform consists of two main functionalities: data cleansing and data fusion. Data cleansing involves importing data from relational databases into the platform, performing iterative cleansing using rules from the rule library based on data feature analysis, and ultimately producing cleaned data that satisfies user requirements. Data fusion involves merging data with similar keywords together. Temporary storage is done using Kafka, data search and analysis using Elasticsearch, and final storage using HDFS.
\item Main Responsibilities: Overall project coordination, project architecture, source code module structure, data search and analysis module, task scheduling module.
\item Tools and Key Technologies: Elasticsearch, Quartz, Kafka, Spring Boot, HDFS, Spark
\item Achievements: Successfully completed the data cleansing platform, resulting in high customer satisfaction.
\end{itemize}
Tianhe Big Data Analysis Platform
\begin{itemize}
\item Project Introduction: Users import different types of data - structured and unstructured - into the Tianhe Big Data Analysis Platform. The platform provides users with basic features such as search, associative graph analysis, and geographical information data analysis through processing and analysis. It also offers customized services, such as the creation of sub-projects on top of Tianhe - "Data Cube," which provides functions including data entry, custom search, data browsing, associative analysis, map analysis, and task management.
\item Main Responsibilities: Designing index structures for different types of data, providing index and search access, leading the coding and optimization of the entire search module; importing and analyzing unstructured data (including doc, pdf, email); file server; geographical information data search and statistics; mavenization of the entire project; design and implementation of the monitoring system.
\item Tools and Key Technologies: Elasticsearch, Netty, Tika, Spark, Redis, Kafka
\item Achievements: Successfully achieved unified analysis and visual presentation of various data sources.
\end{itemize}

\datedsubsection{\textbf{Baidu Netcom Technology Co., Ltd.}, Beijing}{January 2014 -- June 2014}
\role{Intern}{}
Knowledge Search Department, Baidu Netcom Technology Co., Ltd., Beijing
\begin{itemize}
\item Overview: The data processing platforms I worked with can be categorized into three types: existing big data platforms in other departments, partially built big data platforms in the encyclopedia department, and relational database systems. I completed tasks of providing useful data to other data requesters using the existing data platforms, and subsequently participated in building the Baidu Encyclopedia big data system.
\item Main Responsibilities: Developed the internship project "Album Management System" to understand Baidu's LAMP development architecture and environment; utilized big data platforms from other departments for data analysis tasks; tested the big data platform in our department and extracted feature data from it; provided structured data that met the requirements to PM through scripting (performance); wrote scripts for periodic tasks.
\item Tools and Key Technologies: PHP, MySQL, Hive, Hadoop, Shell
\item Achievements: Provided data support for PMs to analyze user behavior during the internship, and conducted testing on the initially constructed big data system.
\end{itemize}

\section{\faHeartO\ Honors and Awards}
\datedline{\textit{Beihang University Master's Scholarships (Second Prize) twice}}{2011 -- 2012}
\datedline{Northeast Normal University First Prize Scholarships twice, Second Prize Scholarships twice, Outstanding Student once. Also awarded the National Inspirational Scholarship and Third Prize in Jilin Province ACM IC/PC Competition.}{2006-2010}

\section{\faInfo\ Research Achievements}
% increase linespacing [parsep=0.5ex]
\begin{itemize}[parsep=0.5ex]
\item Zhang Ren, Jing Kai. Technical Invention Patent, Architecture for Making Unstructured Documents Searchable, Patent Number (LZ1605815CN01).
\item Jing Kai, Zhang Ren. Technical Invention Patent, Automatic Scalable Data Searchable Implementation for Relational Databases under Uncertain Language Environment Conditions, Patent Number (LZ1605814CN01).
\item Zhang Ren, Li Shuai, Wang Lili, Hao Aimin, Pan Junjun. National Invention Patent, Real-Time Realistic Rendering Method for Human Heart, Patent Number (201410768217.7).
\item Zhang Ren, Yi Zhike, Hao Aimin, Zhen Chenglizhao. A Real-Time Realistic Rendering Method for Active Human Heart. ICISCE 2015.
\end{itemize}

%% Reference
%\newpage
%\bibliographystyle{IEEETran}
%\bibliography{mycite}
\end{document}
